---
title: "Trends in CAPE/Shear/CIN"
author: "James Elsner"
date: "12/4/2018"
output: github_notebook
editor_options:
  chunk_output_type: console
---

Compare CAPE, CIN, Shear using two different samples. Sample one are all days with at least 30 tornadoes. Sample two are a random sample of days with fewer than 10 tornadoes.

Correlation screen using FRQ as the number of tornadoes on a big day and INT as the average per tornado energy dissipation on the big day.

## Part 1: Tornado data

Set working directory and load packages.
```{r}
library(tidyverse)
library(lubridate)
library(sf)
library(tmap)
library(USAboundaries)
library(rgeos)
```

Download the tornado data from the Storm Prediction Center (SPC) http://www.spc.noaa.gov/gis/svrgis/
```{r}
download.file(url = "http://www.spc.noaa.gov/gis/svrgis/zipped/1950-2017-torn-initpoint.zip",
              destfile = "tornado.zip", mode = "wb")
unzip("tornado.zip")
```

Load the shapefile into R.
```{r}
Torn.sf <- read_sf(dsn = "1950-2017-torn-initpoint", 
                   layer = "1950-2017-torn-initpoint")
```

Remove tornadoes occurring in Hawaii, Alaska, and Puerto Rico and those occurring before 1994. That year marks the beginning of comprehensive WSR-88D radar coverage. For missing EF ratings use the modification rules (if/else) defined here: https://www.spc.noaa.gov/wcm/OneTor_F-scale-modifications.pdf
```{r}
Torn.sf <- Torn.sf %>%
  filter(yr >= 1994,
         !st %in% c("AK", "PR", "HI")) %>%
  mutate(mag = ifelse(mag == -9 & len <= 5, 0, mag),
         mag = ifelse(mag == -9 & len > 5, 1, mag))
```

Add a data/time column also add columns for path length, width, and area in metric units. Leave the time zone as native CDT. Create a convective day (6AM to 6AM) column taking hours 00:00:00 -> 05:59:59 and assigning it to the previous date (this associates the previous day's date to tornadoes occurring up to 6 hours after local midnight).
```{r}
Torn.sf <- Torn.sf %>%
  mutate(dy = format(as.Date(date, format="%Y-%m-%d"), "%d"),
         DateTime = as.POSIXct(paste(yr, mo, dy, time), format = "%Y%m%d%H:%M:%S"),
         Hour = hour(DateTime),
         Year = year(DateTime),
         cDateTime = DateTime - as.difftime(6, unit = "hours"),
         cDate = as.Date(as_datetime(ifelse(Hour < 6, (DateTime - 86400), cDateTime), tz = Sys.timezone())),
         Length = len * 1609.34,
         Length = ifelse(Length == 0, min(Length[Length > 0]), Length), #takes care of zero length
         Width = wid * .9144,
         Width = ifelse(Width == 0, min(Width[Width > 0]), Width), #takes care of zero width
         Width = ifelse(Year >= 1995, Width * pi/4, Width), #takes care of change: avg to max
         cas = inj + fat,
         AreaPath = Length * Width,
         Ma = factor(month.abb[mo], levels = month.abb[1:12])) 
max(Torn.sf$yr)
```

The geometry type is a `POINT`. Each tornado is represented as a single start point. 

Add energy dissipation per tornado.
```{r}
perc <- c(1, 0, 0, 0, 0, 0, 
          .772, .228, 0, 0, 0, 0,
          .616, .268, .115, 0, 0, 0,
          .529, .271, .133, .067, 0, 0,
          .543, .238, .131, .056, .032, 0,
          .538, .223, .119, .07, .033, .017)
percM <- matrix(perc, ncol = 6, byrow = TRUE)
threshW <- c(29.06, 38.45, 49.62, 60.8, 74.21, 89.41)
midptW <- c(diff(threshW)/2 + threshW[-length(threshW)], threshW[length(threshW)] + 7.5)
ef <- Torn.sf$mag + 1
EW3 <- numeric()
for(i in 1:length(ef)) EW3[i] = midptW^3 %*% percM[ef[i], ]
Torn.sf <- Torn.sf %>%
  mutate(ED = EW3 * AreaPath)
```

Maps of tornadoes by month.

```{r}
sts <- state.name[!state.name %in% c("Alaska", "Hawaii")]
stateBorders <- us_states(states = sts)

tm_shape(world) +
  tm_polygons() +
tm_shape(world[world$name_long == "United States", ]) +
  tm_polygons(col = "white") +
tm_shape(Torn.sf) +
  tm_dots() + 
  tm_facets(by = "mo", as.layers = TRUE) +
tm_shape(stateBorders, projection = "laea_NA", is.master = TRUE) + 
  tm_borders() +
  tm_compass(type = "arrow", position = c("left", "bottom")) +
  tm_scale_bar(position = c("left", "bottom"), size = .75) +
   tm_style("natural") +
  tm_layout(main.title = "Contiguous U.S. Tornadoes [1994-2017]",
            main.title.position = "center", main.title.size = .85,
            panel.labels = month.name) +
  tm_credits(c(rep("", 11), "Data Source: U.S. SPC"), position = c("right", "bottom"))

```

Determine big days.
```{r}
BigDays.sf <- Torn.sf %>%
  group_by(cDate) %>%
  summarize(nT = n(),
            ATE = sum(ED),
            AvgATE = exp(mean(log(ED)))) %>%
  filter(nT >= 30)
dim(BigDays.sf)
```

Determine medium (and big) days.
```{r}
MedDays.sf <- Torn.sf %>%
  group_by(cDate) %>%
  summarize(nT = n()) %>%
  filter(nT >= 10) 
```

Use a projection that matches the projection of the environmental raster data.
```{r}
BigDays.sfT <- st_transform(BigDays.sf, 
                            crs = "+proj=lcc +lat_1=50 +lat_2=50 +lat_0=50 +lon_0=-107 +x_0=0 +y_0=0 +a=6371200 +b=6371200 +units=m +no_defs")
```

Get state borders and use the `tm_shape()` function.
```{r}
sts <- state.name[!state.name %in% c("Alaska", "Hawaii")]
stateBorders <- us_states(states = sts)

tm_shape(stateBorders) + 
  tm_borders(col = "grey") +
  tm_layout(legend.outside = TRUE) +
tm_shape(BigDays.sfT) +
  tm_dots() 
```

Pecentage of all tornadoes occurring on the big days.
```{r}
sum(BigDays.sfT$nT)/dim(Torn.sf)[1] * 100
```

Obtain the big day hulls and centroids.
```{r}
BigDayHulls.sfT <- st_convex_hull(BigDays.sfT)
BigDayCentroids.sfT <- st_centroid(BigDays.sfT)
Area <- st_area(BigDayHulls.sfT)
BigDayHulls.sfT$Area <- Area
BigDayCentroids.sfT$Area <- Area
```

Check on a map.
```{r}
tm_shape(BigDayHulls.sfT) +
  tm_polygons(alpha = .1) + 
tm_shape(stateBorders, projection = "laea_NA", is.master = TRUE) + 
  tm_borders()
```

Arrange by accumulated tornado energy (ATE).
```{r}
BigDays.sfT %>%
  top_n(ATE, n = 20) %>%
  arrange(desc(ATE))
```

Density plot of ATE.
```{r}
labels <- c("10", "100", "1000","10000", "100000")

ggplot(BigDays.sfT, aes(log10(ATE))) +
  geom_histogram(binwidth = .5, color = "white") +
  scale_x_continuous(breaks = 10:14, labels = labels) +
  xlab("Accumulated Tornado Energy [GW]") +
  ylab("Frequency") +
  theme_minimal()
```

## Part 2: Environmental data on big days

Get environmental data at 18Z (2p local) on the convective day. Set up a vector of URLs as character strings. Data are not available after September 30, 2014.
```{r}
library(lubridate)
df <- BigDayHulls.sfT %>%
  filter(cDate <= as.Date("2014-09-30")) %>%
  mutate(Yr = year(cDate),
         Mo = month(cDate),
         Month = format(cDate, "%m"), # this is needed to preserve the leading zeros
         Day = format(cDate, "%d"), 
         YrMo = paste0(Yr, Month),
         YrMoDa = paste0(YrMo, Day),
         slug2 = paste0(YrMo, "/", YrMoDa, "/", "narr-a_221_", YrMoDa, "_1800_000.grb"),
         slug = paste0("https://nomads.ncdc.noaa.gov/data/narr/", slug2)) 
slug <- df$slug
```

Download the grib files. ~ 2 hours to download 300 grb file.
```{r}
for(i in 1:length(slug)){
    download.file(slug[i], paste0("Archive/NARRdata", i, ".grb"), mode = "wb")
    }
```

Read the grib files as raster bricks and assign the CAPE and helicity variables to separate raster layers. Extract the average (and extreme) environmental values within each of the big days in large groups hulls. https://nomads.ncdc.noaa.gov/data/narr/201104/20110427/narr-a_221_20110427_0000_000.inv

323:HLCY:3000-0 m above gnd:kpds=190,106,7680:anl:winds are N/S:"Storm relative helicity [m^2/s^2] [J/kg]
324:USTM:6000-0 m above gnd:kpds=196,106,15360:anl:winds are N/S:"u-component of storm motion [m/s]
325:VSTM:6000-0 m above gnd:kpds=197,106,15360:anl:winds are N/S:"v-component of storm motion [m/s]

```{r}
library(raster)
aCAPE <- numeric()
aHLCY <- numeric()
aCIN <- numeric()
aUSTM <- numeric()
aVSTM <- numeric()
aBS <- numeric()
mCAPE <- numeric()
mHLCY <- numeric()
mCIN <- numeric()
mUSTM <- numeric()
mVSTM <- numeric()
mBS <- numeric()

for(i in 1:length(slug)){
  print(i)
  rb <- brick(paste0("Archive/NARRdata", i, ".grb"))
  CAPE.rl <- raster(rb, layer = 375)
  HLCY.rl <- raster(rb, layer = 323)
  CIN.rl <- raster(rb, layer = 376)
  USTM.rl <- raster(rb, layer = 324)
  VSTM.rl <- raster(rb, layer = 325)
  BS.rl <- sqrt(USTM.rl^2 + VSTM.rl^2)
  aCAPE <- c(aCAPE, as.numeric(extract(CAPE.rl, df[i, ], fun = mean)))
  aHLCY <- c(aHLCY, as.numeric(extract(HLCY.rl, df[i, ], fun = mean)))
  aCIN <- c(aCIN, as.numeric(extract(CIN.rl, df[i, ], fun = mean)))
  aUSTM <- c(aUSTM, as.numeric(extract(USTM.rl, df[i, ], fun = mean)))
  aVSTM <- c(aVSTM, as.numeric(extract(VSTM.rl, df[i, ], fun = mean)))
  aBS <- c(aBS, as.numeric(extract(BS.rl, df[i, ], fun = mean)))
  mCAPE <- c(mCAPE, as.numeric(extract(CAPE.rl, df[i, ], fun = max)))
  mHLCY <- c(mHLCY, as.numeric(extract(HLCY.rl, df[i, ], fun = max)))
  mCIN <- c(mCIN, as.numeric(extract(CIN.rl, df[i, ], fun = min)))
  mUSTM <- c(mUSTM, as.numeric(extract(USTM.rl, df[i, ], fun = max)))
  mVSTM <- c(mVSTM, as.numeric(extract(VSTM.rl, df[i, ], fun = max)))
  mBS <- c(mBS, as.numeric(extract(BS.rl, df[i, ], fun = max)))
}
```

Add environmental data values to the group day means data frame.
```{r}
df$aCAPE <- aCAPE
df$aHLCY <- aHLCY
df$aCIN <- -aCIN
df$aUSTM <- aUSTM
df$aVSTM <- aVSTM
df$aBS <- aBS
df$mCAPE <- mCAPE
df$mHLCY <- mHLCY
df$mCIN <- -mCIN
df$mUSTM <- mUSTM
df$mVSTM <- mVSTM
df$mBS <- mBS
```

Save the `df` so we can work on the models without running all the code above.
```{r}
save(df, file = "df.RData")
#load("df.RData")
```

Trends. Leave off 2014 because there is no environmental data after September 2014.
```{r}
p1 <- df %>%
  filter(Yr < 2014) %>%
  group_by(Yr) %>%
  summarize(AnnualAvg = mean(aHLCY, na.rm = TRUE)) %>%
ggplot(., aes(x = Yr, y = AnnualAvg)) +
  geom_point() + geom_smooth(method = lm, color = "black", size = .35) +
  scale_y_continuous(limits = c(0, NA)) +
  scale_x_continuous(breaks = seq(1994, 2013, 4)) +
  ylab("Storm Relative Helicity\n [J/kg]") + xlab("Year") +
  theme_minimal() +
  ggtitle("A")

p2 <- df %>%
  filter(Yr < 2014) %>%
  group_by(Yr) %>%
  summarize(AnnualAvg = mean(aCIN, na.rm = TRUE)) %>%
ggplot(., aes(x = Yr, y = -AnnualAvg)) +
  geom_point()  + geom_smooth(method = lm, color = "black", size = .35) +
  scale_y_continuous(limits = c(0, NA)) +
  scale_x_continuous(breaks = seq(1994, 2013, 4)) +
  ylab("Convective Inhibition\n [J/kg]") + xlab("Year") +
  theme_minimal() +
  ggtitle("B")

p3 <- df %>%
  filter(Yr < 2014) %>%
  group_by(Yr) %>%
  summarize(AnnualAvg = mean(aBS, na.rm = TRUE)) %>%
ggplot(., aes(x = Yr, y = AnnualAvg)) +
  geom_point() + geom_smooth(method = lm, color = "black", size = .35) +
  scale_y_continuous(limits = c(0, NA)) +
  scale_x_continuous(breaks = seq(1994, 2013, 4)) +
  ylab("Bulk Shear\n [m/s]") + xlab("Year") +
  theme_minimal() +
  ggtitle("C")

p4 <- df %>%
  filter(Yr < 2014) %>%
  group_by(Yr) %>%
  summarize(AnnualAvg = mean(aCAPE, na.rm = TRUE)) %>%
ggplot(., aes(x = Yr, y = AnnualAvg)) +
  geom_point() + geom_smooth(method = lm, color = "black", size = .35) +
  scale_y_continuous(limits = c(0, NA)) +
  scale_x_continuous(breaks = seq(1994, 2013, 4)) +
  ylab("CAPE\n [J/kg]") + xlab("Year") +
  theme_minimal() +
  ggtitle("D")

library(patchwork)
p1 + p2 + p3 + p4
```

Increasing CIN helps explain the increasing percentage of all tornadoes occurring on days with many tornadoes (Elsner et al. 2014). Increasing SRH is consistent with increasing shear.

## Part 3: Environmental data on randomly chosen days with fewer than 10 tornadoes

Choose random days not in the list of big days. Start with a sequence of all days in the interval. Remove big days from the sequence. Then use the frequency of big day months as the weights.
```{r}
AllDates <- seq(ymd('1994-01-01'),
                ymd('2014-09-30'),
                by = '1 day')
Months <- month(BigDays.sfT$cDate)
Weights <- as.numeric(table(Months))/length(Months)
Weights <- c(Weights[1:6], 0, Weights[7:11])
```

Randomly choose months with a frequency that matches the frequency of big days. Randomly choose years and days then combine to make a date object. Remove any dates that are have ten or more tornadoes (`MedDays.sf`).
```{r}
set.seed(0112)
rMo <- sample.int(n = 12, 
                  size = 200, 
                  replace = TRUE, 
                  prob = Weights)
rYr <- sample(x = seq(1994, 2013), 
              size = 200, 
              replace = TRUE)
rDa <- sample.int(n = 28,
                  size = 200,
                  replace = TRUE)
rDates <- as.Date(paste0(rYr, "-", rMo, "-", rDa))
rDates <- rDates[!rDates %in% MedDays.sf$cDate]
```

Get the url slugs for these non big days.
```{r}
Yr <- year(rDates)
Month <- format(rDates, "%m")
Day <- format(rDates, "%d")
YrMo <- paste0(Yr, Month)
YrMoDa <- paste0(YrMo, Day)
slug2 <- paste0(YrMo, "/", YrMoDa, "/", "narr-a_221_", YrMoDa, "_1800_000.grb")
slug <- paste0("https://nomads.ncdc.noaa.gov/data/narr/", slug2)
```

Get usa boundary as a simple feature.
```{r}
usBoundary <- us_states(resolution = "low") %>%
  filter(!stusps %in% c("AK", "HI", "PR", "DC")) %>%
  st_union() %>%
  st_transform(crs = "+proj=lcc +lat_1=50 +lat_2=50 +lat_0=50 +lon_0=-107 +x_0=0 +y_0=0 +a=6371200 +b=6371200 +units=m +no_defs")
usB <- as(usBoundary, "Spatial")
```

Get tornado alley as a simple feature. Nebraska, Kansas, Oklahoma, Texas, Arkansas, Missouri, Illinois, Indiana, Ohio, Kentucky, Tennessee, Louisiana, Mississippi, Alabama, Georgia.
```{r}
usTA <- us_states(resolution = "low") %>%
  filter(stusps %in% c("NE", "KS", "OK", "TX", "AR", "MO", "IL", "IA",
                       "IN", "OH", "KY", "TN", "LA", "MS", "AL", "GA" )) %>%
  st_union() %>%
  st_transform(crs = "+proj=lcc +lat_1=50 +lat_2=50 +lat_0=50 +lon_0=-107 +x_0=0 +y_0=0 +a=6371200 +b=6371200 +units=m +no_defs")
usB <- as(usTA, "Spatial")
```

Download the 18Z convective variables from NARR.
```{r}
for(i in 1:length(slug)){
    download.file(slug[i], paste0("Archive2/NARRdata", i, ".grb"), mode = "wb")
    }
```

Import grids and extract environmental variables over the US.
```{r}
library(raster)
aCAPE <- numeric()
aHLCY <- numeric()
aCIN <- numeric()
aUSTM <- numeric()
aVSTM <- numeric()
aBS <- numeric()
aPW <- numeric()
mCAPE <- numeric()
mHLCY <- numeric()
mCIN <- numeric()
mUSTM <- numeric()
mVSTM <- numeric()
mBS <- numeric()
mPW <- numeric()

for(i in 1:length(slug)){
  print(i)
  rb <- brick(paste0("Archive2/NARRdata", i, ".grb"))
  CAPE.rl <- raster(rb, layer = 375)
#  CAPE.rl <- raster(rb, layer = 315)  # sfc based 
  CIN.rl <- raster(rb, layer = 376)
#  CIN.rl <- raster(rb, layer = 316)  # sfc based 
  HLCY.rl <- raster(rb, layer = 323)
  USTM.rl <- raster(rb, layer = 324)
  VSTM.rl <- raster(rb, layer = 325)
  BS.rl <- sqrt(USTM.rl^2 + VSTM.rl^2)
  PW.rl <- raster(rb, layer = 317)
  aCAPE <- c(aCAPE, as.numeric(extract(CAPE.rl, usB, fun = mean)))
  aHLCY <- c(aHLCY, as.numeric(extract(HLCY.rl, usB, fun = mean)))
  aCIN <- c(aCIN, as.numeric(extract(CIN.rl, usB, fun = mean)))
  aUSTM <- c(aUSTM, as.numeric(extract(USTM.rl, usB, fun = mean)))
  aVSTM <- c(aVSTM, as.numeric(extract(VSTM.rl, usB, fun = mean)))
  aBS <- c(aBS, as.numeric(extract(BS.rl, usB, fun = mean)))
  aPW <- c(aPW, as.numeric(extract(PW.rl, usB, fun = mean)))
  mCAPE <- c(mCAPE, as.numeric(extract(CAPE.rl, usB, fun = max)))
  mHLCY <- c(mHLCY, as.numeric(extract(HLCY.rl, usB, fun = max)))
  mCIN <- c(mCIN, as.numeric(extract(CIN.rl, usB, fun = min)))
  mUSTM <- c(mUSTM, as.numeric(extract(USTM.rl, usB, fun = max)))
  mVSTM <- c(mVSTM, as.numeric(extract(VSTM.rl, usB, fun = max)))
  mBS <- c(mBS, as.numeric(extract(BS.rl, usB, fun = max)))
  mPW <- c(mPW, as.numeric(extract(PW.rl, usB, fun = max)))
}
```

```{r}
dfTA <- data.frame(Yr, rDates, aCAPE, aCIN = -aCIN, aHLCY, aUSTM, aVSTM, aBS, aPW,
                               mCAPE, mCIN = -mCIN, mHLCY, mUSTM, mVSTM, mBS, mPW)

save(dfTA, file = "dfTA.RData")
#load("df.RData")
```

Annual trends.
```{r}
p5 <- dfTA %>%
  group_by(Yr) %>%
  summarize(AnnualAvg = mean(mHLCY),
            nD = n()) %>%
ggplot(., aes(x = Yr, y = AnnualAvg, weight = nD)) +
  geom_point() + geom_smooth(method = lm, color = "black", size = .35) +
  scale_y_continuous(limits = c(0, NA)) +
  scale_x_continuous(breaks = seq(1994, 2013, 4)) +
  ylab("Storm Relative Helicity\n [J/kg]") + xlab("Year") +
  theme_minimal() +
  ggtitle("A")

p6 <- dfTA%>%
  group_by(Yr) %>%
  summarize(AnnualAvg = mean(mCIN),
            nD = n()) %>%
ggplot(., aes(x = Yr, y = AnnualAvg, weight = nD)) +
  geom_point() + geom_smooth(method = lm, color = "black", size = .35) +
  scale_y_continuous(limits = c(0, NA)) +
  scale_x_continuous(breaks = seq(1994, 2013, 4)) +
  ylab("Convective Inhibition\n [J/kg]") + xlab("Year") +
  theme_minimal() +
  ggtitle("B")

p7 <- dfTA %>%
  group_by(Yr) %>%
  summarize(AnnualAvg = mean(mBS),
            nD = n()) %>%
ggplot(., aes(x = Yr, y = AnnualAvg, weight = nD)) +
  geom_point() + geom_smooth(method = lm, color = "black", size = .35) +
  scale_y_continuous(limits = c(0, NA)) +
  scale_x_continuous(breaks = seq(1994, 2013, 4)) +
  ylab("Bulk Shear\n [m/s]") + xlab("Year") +
  theme_minimal() +
  ggtitle("C")

p8 <- dfTA %>%
  group_by(Yr) %>%
  summarize(AnnualAvg = mean(mCAPE),
            nD = n()) %>%
ggplot(., aes(x = Yr, y = AnnualAvg, weight = nD)) +
  geom_point() + geom_smooth(method = lm, color = "black", size = .35) +
  scale_y_continuous(limits = c(0, NA)) +
  scale_x_continuous(breaks = seq(1994, 2013, 4)) +
  ylab("CAPE\n [J/kg]") + xlab("Year") +
  theme_minimal() +
  ggtitle("D")


p5 + p6 + p7 + p8 
```

```{r}
dfTA %>%
  group_by(Yr) %>%
  summarize(AnnualAvg = mean(mPW),
            nD = n()) %>%
ggplot(., aes(x = Yr, y = AnnualAvg, weight = nD)) +
  geom_point() + geom_smooth(method = lm, color = "black", size = .35) +
  scale_y_continuous(limits = c(0, NA)) +
  scale_x_continuous(breaks = seq(1994, 2013, 4)) +
  ylab("PW") + xlab("Year") +
  theme_minimal()
```

The surface temperature response to greenhouse gas forcing displays a pattern of polar-amplified warming (1,2,3) particularly in the Northern Hemisphere. 

1. Manabe, S. & Wetherald, R. The effects of doubling the CO2 concentrations on the climate of a general circulation model. J. Atmos. Sci. 32, 3–15 (1975).
2. Holland, M. M. & Bitz, C. M. Polar amplification of climate change in coupled models. Clim. Dynam. 21, 221–232 (2003).
3. Bintanja, R., Graversen, R. G. & Hazeleger, W. Arctic winter warming amplified by the thermal inversion and consequent low infrared cooling to space. Nat. Geosci. 4, 758–761 (2011).

We see an increase in CAPE due to increased warming and moistening of the atmosphere and a decrease in shear throughout the midlatitudes from polar amplification of the warming.

This is the changing background state. On big tornado days, shear is increasing and so is CIN.

```{r}
Days <- as.data.frame(df) %>%
  group_by(Yr) %>%
  summarize(AnnualAvg = mean(aBS),
            nD = n())
Days$Type <- "Big Days"

Days2 <- dfTA %>%
  group_by(Yr) %>%
  summarize(AnnualAvg = mean(aBS),
            nD = n())
Days2$Type <- "Other Days"

Days <- rbind(Days, Days2)

ggplot(Days, aes(x = Yr, y = AnnualAvg, weight = nD, col = Type)) +
  geom_point() + 
  geom_smooth(method = lm, size = .35) +
  scale_y_continuous(limits = c(0, NA)) +
  scale_x_continuous(breaks = seq(1994, 2013, 4)) +
  ylab("Bulk Shear [m/s]") + xlab("Year") +
  scale_color_discrete(name = "Tornadoes", labels = c("Thirty or more", "Fewer than ten")) +
  theme_minimal()
```

## Part 4: Correlation screen 

See FreqInt.Rmd

```{r}
df2 <- df %>%
  filter(Mo < 3 | Mo > 6)
```

```{r}
theta <- seq(1, 180, by = 1)
r <- NULL ; pval <- NULL
for (k in theta){
  C1 <- cos(k * pi/180)
  C2 <- sin(k * pi/180)
  Tclim <- C1 * scale(df$nT) + C2 * scale(df$AvgATE)
#  Eclim <- C1 * scale(df$mCAPE) + C2 * scale(df$mBS)
  ctest = cor.test(Tclim, df$mCAPE)
#  ctest = cor.test(Eclim, df$AvgATE)
  r <- c(r, as.numeric(ctest$estimate))
  pval <- c(pval, as.numeric(ctest$p.value))
}
range(r)
```

Plot
```{r}
signif = .05
plot(-10, -10, xlim=c(-1, 1.3), ylim=c(-1, 1.3), axes=FALSE, xlab='', ylab='', main='')

i = seq(0, 360, .5)
Outl = cbind(1 * cos(i * pi/180), 1 * sin(i * pi/180))
Innl = cbind(.5 * cos(i * pi/180), .5 * sin(i * pi/180))

polygon(Outl[, 1], Outl[, 2], border=colors()[229], col='white', lwd=2)
polygon(Innl[, 1], Innl[, 2], border=colors()[229], col=NULL, lwd=2)

Line.xcord = c(-1, 1, NA, 0, 0, NA, -cos(pi/4), cos(pi/4), NA, -cos(pi/4), cos(pi/4))
Line.ycord = c(0, 0, NA, -1, 1, NA, sin(pi/4), -sin(pi/4), NA, -sin(pi/4), sin(pi/4))
lines(Line.xcord, Line.ycord, col=colors()[229], lwd=1)

text(par('usr')[2] - 0.29, 0.0, srt=0, adj = 0, labels = 'INT', xpd = TRUE, cex=1.3) 
text(par('usr')[2] - 0.6, 0.81, srt=0, adj = 0, labels = 'ACT', xpd = TRUE, cex=1.3)
text(par('usr')[2] - 1.52, 1.17, srt=0, adj = 0, labels = 'FRQ', xpd = TRUE, cex=1.3)
text(par('usr')[2] - 0.6, -0.81, srt=0, adj = 0, labels = 'EINT', xpd = TRUE, cex=1.3)
text(0,0.55, '0.5', cex=1.4, col=colors()[229])
text(0,1.05, '1.0', cex=1.4, col=colors()[229])

dg = theta
polygon(r * cos(dg * pi/180), r * sin(dg * pi/180), border="#ff9900", lwd=7, col=NULL)
r2 = c(r[which.max(pval):length(pval)], r[1:(which.max(pval) - 1)])
pval2 = c(pval[which.max(pval):length(pval)], pval[1:(which.max(pval) - 1)])
dg2 = c(dg[which.max(pval):length(pval)], dg[1:(which.max(pval) - 1)])
lines(r2[pval2 <= signif] * cos(dg2[pval2 <= signif] * pi/180),
      r2[pval2 <= signif] * sin(dg2[pval2 <= signif] * pi/180), col="#cc3300", lwd=7) 
```